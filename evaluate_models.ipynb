{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from vit_pytorch import ViT\n",
    "from dqn_agent import *\n",
    "from train_test_agent import *\n",
    "from viz import *\n",
    "from rl_env import *\n",
    "from vit import *\n",
    "from tinyimagenet import TinyImageNet\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "# import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_save_path        = \"./model\"\n",
    "results_save_path_agent = \"./result\"\n",
    "dataset_path            = \"./dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "\n",
    "device_their = torch.device(\"cpu\")\n",
    "device_our = torch.device(\"cpu\")\n",
    "device_baseline = torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "## Define what dataset we're using.\n",
    "datasets = [\"tinyimagenet\", \"cifar10\"]\n",
    "dataset_idx = 1 #Toggle for choosing dataset\n",
    "dataset_name = datasets[dataset_idx]\n",
    "use_subset = False\n",
    "subset_classes = 20\n",
    "load_indices = True\n",
    "\n",
    "if dataset_idx == 0:\n",
    "    img_size = 64\n",
    "elif dataset_idx == 1:\n",
    "    img_size = 32\n",
    "\n",
    "img_size = 32\n",
    "patch_size = 4\n",
    "att_dim = 128\n",
    "classes = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "batch_size = 128\n",
    "\n",
    "\n",
    "# Create dataset loaders and transforms\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(img_size, padding=8),\n",
    "    transforms.Resize(img_size),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=TinyImageNet.mean, std=TinyImageNet.std),\n",
    "])\n",
    "\n",
    "transform_validation = transforms.Compose([\n",
    "    transforms.Resize(img_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=TinyImageNet.mean, std=TinyImageNet.std),\n",
    "])\n",
    "\n",
    "# Prepare and load TinyImageNet dataset\n",
    "if (dataset_idx == 0):\n",
    "    trainset_pure = TinyImageNet(Path(\"~/.torchvision/tinyimagenet/\"),split=\"train\",imagenet_idx=False)\n",
    "    trainset =      TinyImageNet(Path(\"~/.torchvision/tinyimagenet/\"),split=\"train\",imagenet_idx=False, transform=transform_train)\n",
    "    validationset = TinyImageNet(Path(\"~/.torchvision/tinyimagenet/\"),split=\"val\",imagenet_idx=False, transform=transform_validation)\n",
    "\n",
    "# Prepare/download CIFAR10 dataset\n",
    "if (dataset_idx == 1):\n",
    "    trainset_pure = torchvision.datasets.CIFAR10(root=dataset_path, train=True, download=True)\n",
    "    trainset =      torchvision.datasets.CIFAR10(root=dataset_path, train=True, download=True, transform=transform_train)\n",
    "    validationset = torchvision.datasets.CIFAR10(root=dataset_path, train=False, download=True, transform=transform_validation)\n",
    "\n",
    "# Create empty training and validation loaders\n",
    "train_loader = None\n",
    "validation_loader = None\n",
    "test_loader = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model and Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_batch_size = 8\n",
    "buffer_size = 64\n",
    "\n",
    "gamma = 0.95\n",
    "\n",
    "eps_start = 1\n",
    "eps_end = 0.01\n",
    "eps = eps_start\n",
    "eps_decay = 20000\n",
    "\n",
    "lr = 0.01\n",
    "\n",
    "tau = 0.1\n",
    "update_every = 2\n",
    "\n",
    "get_reward_every = 10\n",
    "\n",
    "ourPretrained = False  # Pretrained with CIFAR10 weights, that is.\n",
    "pretrained = False\n",
    "verbose = False     # Whether to print out training progress or not\n",
    "\n",
    "max_reward = 10\n",
    "alpha = 0.2 # Where alpha defines weight of loss reward, and 1-alpha defines weight of patch (time) reward\n",
    "# loss_weight = max_reward*(alpha)  # TODO THIS ONE TOO\n",
    "# time_weight = max_reward*(1-alpha) # TODO PLAY WITH THESE VALUES A LOT\n",
    "loss_weight = alpha\n",
    "time_weight = 1-alpha\n",
    "\n",
    "# Input image to DQN agent's  Q-network\n",
    "# for CIFAR, lowres=16, fullres=32\n",
    "# For TinyImageNet, halfres = 32, fullres = 64\n",
    "dqn_img_w = img_size // 2\n",
    "dqn_input_channels = 3\n",
    "dqn_input_size = dqn_img_w*dqn_img_w*dqn_input_channels # input to DQN (mlp type)a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n"
     ]
    }
   ],
   "source": [
    "# This is the number of patches along the width/height of the square image.\n",
    "# TODO refactor...img_size is defined WAY up in the notebook, right before dataset loader initialization\n",
    "patch_width = 4\n",
    "# patch_size = int(img_size/patch_width)\n",
    "patch_size = patch_width # This is what you get when you work with someone else's code and don't want to rewrite everything.\n",
    "\n",
    "\n",
    "total_patches = int((img_size/patch_width)**2)\n",
    "print(total_patches)\n",
    "n_patch_selected = int(total_patches*(40/64)) # From the paper\n",
    "\n",
    "\n",
    "att_dim = 128\n",
    "\n",
    "epochs = 20\n",
    "save_every = 10\n",
    "learning_rate = 0.001\n",
    "\n",
    "\n",
    "# Definte the three transformer models.\n",
    "SimpleAgentViTnet = SimpleAgentViT(\n",
    "    image_size = img_size,\n",
    "    patch_size = patch_size,\n",
    "    num_classes = len(classes),\n",
    "    dim = att_dim,\n",
    "    depth = 6,\n",
    "    heads = 16,\n",
    "    mlp_dim = 512\n",
    ")\n",
    "\n",
    "OurViTnet = SimpleAgentViT(\n",
    "    image_size = img_size,\n",
    "    patch_size = patch_size,\n",
    "    num_classes = len(classes),\n",
    "    dim = att_dim,\n",
    "    depth = 6,\n",
    "    heads = 16,\n",
    "    mlp_dim = 512\n",
    ")\n",
    "\n",
    "BaselineSimpleViT = SimpleViT(\n",
    "    image_size = img_size,\n",
    "    patch_size = patch_size,\n",
    "    num_classes = len(classes),\n",
    "    dim = att_dim,\n",
    "    depth = 6,\n",
    "    heads = 16,\n",
    "    mlp_dim = 512\n",
    ")\n",
    "\n",
    "# Sposta il modello sulla GPU (se disponibile)\n",
    "SimpleAgentViTnet.to(device_their)\n",
    "OurViTnet.to(device_our)\n",
    "BaselineSimpleViT.to(device_baseline)\n",
    "    \n",
    "# definiamo l'ottimizzatore\n",
    "SimpleOptimizer =   optim.Adam(SimpleAgentViTnet.parameters(), lr=learning_rate)\n",
    "OurOptimizer =      optim.Adam(OurViTnet.parameters(), lr=learning_rate)\n",
    "BaselineOptimizer = optim.Adam(BaselineSimpleViT.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create TrainingTesting Agents and Load in Model Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvNet created! Value of pretrained: False\n",
      "ConvNet created! Value of pretrained: False\n"
     ]
    }
   ],
   "source": [
    "testingAgents = dict()\n",
    "\n",
    "##  Create all of the trainingtestingagents, and load in the model parameters where appropriate\n",
    "#env = ViTEnv(ViTnet, total_patches, optimizer, loss_weight, time_weight, device, n_patch_selected)\n",
    "theirEnv = ViTEnv(SimpleAgentViTnet, total_patches, SimpleOptimizer, loss_weight, time_weight, device_their, n_patch_selected, verbose=verbose, total_epochs=epochs)\n",
    "ourEnv = OurViTEnv(OurViTnet, total_patches, OurOptimizer, loss_weight, time_weight, device_our, n_patch_selected, verbose=verbose, total_epochs=epochs)\n",
    "\n",
    "ourModel = OurTrainingTestingAgent(epochs = epochs,\n",
    "                             model = OurViTnet,\n",
    "                             get_reward_every = get_reward_every,\n",
    "                             buffer_batch_size = buffer_batch_size,\n",
    "                             batch_size = batch_size,\n",
    "                             env = ourEnv,\n",
    "                             att_dim = att_dim,\n",
    "                             n_patches = total_patches,\n",
    "                             buffer_size = buffer_size,\n",
    "                             gamma = gamma,\n",
    "                             tau = tau,\n",
    "                             update_every = update_every,\n",
    "                             lr = lr,\n",
    "                             eps_end = eps_end,\n",
    "                             eps_start = eps_start,\n",
    "                             eps_decay = eps_decay,\n",
    "                             train_loader = train_loader,\n",
    "                             validation_loader = validation_loader,\n",
    "                             device = device_our,\n",
    "                             dqn_input_size=dqn_input_size,\n",
    "                             save_every=save_every,\n",
    "                             verbose=verbose,\n",
    "                             pretrained=ourPretrained)\n",
    "\n",
    "theirModel = TrainingTestingAgent(epochs = epochs,\n",
    "                             model = SimpleAgentViTnet,\n",
    "                             get_reward_every = get_reward_every,\n",
    "                             buffer_batch_size = buffer_batch_size,\n",
    "                             batch_size = batch_size,\n",
    "                             env = theirEnv,\n",
    "                             att_dim = att_dim,\n",
    "                             n_patches = total_patches,\n",
    "                             buffer_size = buffer_size,\n",
    "                             gamma = gamma,\n",
    "                             tau = tau,\n",
    "                             update_every = update_every,\n",
    "                             lr = lr,\n",
    "                             eps_end = eps_end,\n",
    "                             eps_start = eps_start,\n",
    "                             eps_decay = eps_decay,\n",
    "                             train_loader = train_loader,\n",
    "                             validation_loader = validation_loader,\n",
    "                             save_every=save_every,\n",
    "                             verbose=verbose,\n",
    "                             device = device_their)\n",
    "\n",
    "baselineModel = SimpleViTTrainingTestingAgent(epochs = epochs,\n",
    "                             batch_size = batch_size, \n",
    "                             model = BaselineSimpleViT,\n",
    "                             train_loader = train_loader, \n",
    "                             validation_loader = validation_loader, \n",
    "                             device = device_baseline,\n",
    "                             optimizer=BaselineOptimizer, \n",
    "                            #  criterion=criterion,\n",
    "                             save_every=save_every, \n",
    "                             verbose=verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the paths to the model files\n",
    "agent_names = [\"ourAgent_qnetwork\", \"theirAgentQNetwork\"]\n",
    "vitnet_names = [\"ourViTNet\", \"theirViTNet\", \"simpleViTNet\"]\n",
    "epochs = [10,20]\n",
    "datasets = [\"cifar10\", \"tinyimagenet\"]\n",
    "dataset = datasets[0]\n",
    "epoch = epochs[-1]\n",
    "path_base = 'model/'\n",
    "timestamp = \"02_03_13\"\n",
    "\n",
    "agent_paths = dict()\n",
    "vitnet_paths = dict()\n",
    "\n",
    "\n",
    "for epoch in epochs:\n",
    "    for i, vitnet in enumerate(vitnet_names):\n",
    "        vitnet_path = path_base + vitnet + \"_epoch_\" + str(epoch) + \"_\" + dataset + \"_\" + timestamp + \".pth\"\n",
    "        if vitnet not in vitnet_paths:\n",
    "            vitnet_paths[vitnet] = {}\n",
    "        vitnet_paths[vitnet][epoch] = vitnet_path\n",
    "\n",
    "    for i, agent in enumerate(agent_names):\n",
    "        agent_path = path_base + agent + \"_epoch_\" + str(epoch) + \"_\" + dataset + \"_\" + timestamp + \".pth\"\n",
    "        if agent not in agent_paths:\n",
    "            agent_paths[agent] = {}\n",
    "        agent_paths[agent][epoch] = agent_path\n",
    "        \n",
    "# for vitnet in vitnet_names:\n",
    "#     vitnet_path = path_base + vitnet + \"_epoch_\" + str(epoch) + \"_\" + dataset + \"_\" + timestamp + \".pth\"\n",
    "#     vitnet_paths.append(vitnet_path)\n",
    "\n",
    "# for agent in agent_names:\n",
    "#     agent_path = path_base + agent + \"_epoch_\" + str(epoch) + \"_\" + dataset + \"_\" + timestamp + \".pth\"\n",
    "#     agent_paths.append(agent_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Patrick\\AppData\\Local\\Temp\\ipykernel_11132\\364438458.py:18: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ourModel.ViTnet.load_state_dict(torch.load(vitnet_paths[vitnet_names[0]][epoch], map_location=device_our))\n",
      "C:\\Users\\Patrick\\AppData\\Local\\Temp\\ipykernel_11132\\364438458.py:19: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  theirModel.ViTnet.load_state_dict(torch.load(vitnet_paths[vitnet_names[1]][epoch], map_location=device_their))\n",
      "C:\\Users\\Patrick\\AppData\\Local\\Temp\\ipykernel_11132\\364438458.py:20: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  baselineModel.ViTnet.load_state_dict(torch.load(vitnet_paths[vitnet_names[2]][epoch], map_location=device_baseline))\n",
      "C:\\Users\\Patrick\\AppData\\Local\\Temp\\ipykernel_11132\\364438458.py:22: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ourModel.agent.q_network.load_state_dict(torch.load(agent_paths[agent_names[0]][epoch], map_location=device_our))\n",
      "C:\\Users\\Patrick\\AppData\\Local\\Temp\\ipykernel_11132\\364438458.py:23: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  theirModel.agent.q_network.load_state_dict(torch.load(agent_paths[agent_names[1]][epoch], map_location=device_their))\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Image' object has no attribute 'unsqueeze'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 30\u001b[0m\n\u001b[0;32m     28\u001b[0m input_imgs \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m random_images:\n\u001b[1;32m---> 30\u001b[0m     attention_scores_all_images\u001b[38;5;241m.\u001b[39mappend(theirModel\u001b[38;5;241m.\u001b[39mViTnet\u001b[38;5;241m.\u001b[39mget_att(\u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device_their)))\n\u001b[0;32m     31\u001b[0m     input_imgs\u001b[38;5;241m.\u001b[39mappend(transforms\u001b[38;5;241m.\u001b[39mToTensor()(img))\n\u001b[0;32m     33\u001b[0m visualize_selected_patches_multiple_images(random_images, input_imgs, ourModel\u001b[38;5;241m.\u001b[39magent, device_our, titles\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOur Model, Epoch \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(epoch) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_images)])\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Image' object has no attribute 'unsqueeze'"
     ]
    }
   ],
   "source": [
    "## Visualize the patch selection and attention scores for the untrained models.\n",
    "## Use functions defined in viz.py\n",
    "# Select a random set of 5 images from the dataset\n",
    "num_images = 5\n",
    "random_indices = np.random.choice(len(trainset_pure), num_images, replace=False)\n",
    "random_images = [trainset_pure[i][0] for i in random_indices]\n",
    "\n",
    "\n",
    "\n",
    "half_norm_transform = transforms.Compose(\n",
    "    [transforms.Resize((img_size // 2, img_size // 2)), \n",
    "    transforms.Normalize(mean=TinyImageNet.mean, std=TinyImageNet.std)]\n",
    "    )\n",
    "\n",
    "\n",
    "# Load the weights for the ViT models and agents for each epoch\n",
    "for epoch in epochs:\n",
    "    ourModel.ViTnet.load_state_dict(torch.load(vitnet_paths[vitnet_names[0]][epoch], map_location=device_our))\n",
    "    theirModel.ViTnet.load_state_dict(torch.load(vitnet_paths[vitnet_names[1]][epoch], map_location=device_their))\n",
    "    baselineModel.ViTnet.load_state_dict(torch.load(vitnet_paths[vitnet_names[2]][epoch], map_location=device_baseline))\n",
    "\n",
    "    ourModel.agent.q_network.load_state_dict(torch.load(agent_paths[agent_names[0]][epoch], map_location=device_our))\n",
    "    theirModel.agent.q_network.load_state_dict(torch.load(agent_paths[agent_names[1]][epoch], map_location=device_their))\n",
    "\n",
    "    # Visualize the patch selection and attention scores for the untrained models\n",
    "    # Visualize patch selection for each model.\n",
    "    attention_scores_all_images = []\n",
    "    input_imgs = []\n",
    "\n",
    "    for img in random_images:\n",
    "        # Preprocess the image for the model, depending on the dataset and architecture type.\n",
    "        if dataset_idx == 0:\n",
    "            input_img = half_norm_transform(img)\n",
    "        elif dataset_idx == 1:\n",
    "            input_img = transforms.ToTensor()(img)\n",
    "            img = transforms.ToTensor()(img)\n",
    "\n",
    "        attention_scores_all_images.append(theirModel.ViTnet.get_att(img.unsqueeze(0).to(device_their)))\n",
    "        input_imgs.append(transforms.ToTensor()(input_img))\n",
    "\n",
    "    visualize_selected_patches_multiple_images(random_images, input_imgs, ourModel.agent, device_our, titles=[\"Our Model, Epoch \" + str(epoch) for i in range(num_images)])\n",
    "    visualize_selected_patches_multiple_images(random_images, attention_scores_all_images, theirModel.agent, device_their, titles=[\"Their Model, Epoch \" + str(epoch) for i in range(num_images)])\n",
    "    # visualize_selected_patches_multiple_images(random_images, baselineModel.agent, device_baseline, title=[\"Baseline Model, Epoch \" + str(epoch) for i in range(len(num_images))])\n",
    "\n",
    "    # Start with ourModel\n",
    "    for img in random_images:\n",
    "        if dataset_idx == 0:\n",
    "            input_img = half_norm_transform(img)\n",
    "        elif dataset_idx == 1:\n",
    "            input_img = transforms.ToTensor()(img)\n",
    "            img = transforms.ToTensor()(img)\n",
    "        visualize_selected_patches(img, input_img, ourModel.agent, device_our, title=\"Our Model, Epoch \" + str(epoch))\n",
    "        \n",
    "    # Now do theirModel\n",
    "    for img in random_images:   \n",
    "        if dataset_idx == 0:\n",
    "            input_img = half_norm_transform(img)\n",
    "        elif dataset_idx == 1:\n",
    "            input_img = transforms.ToTensor()(img)\n",
    "            img = transforms.ToTensor()(img)\n",
    "        attention_scores = theirModel.ViTnet.get_att(input_img.unsqueeze(0).to(device_their))    \n",
    "        visualize_selected_patches(img, attention_scores, theirModel.agent, device_their, title=\"Their Model, Epoch \" + str(epoch))\n",
    "\n",
    "\n",
    "        # visualize_attention_scores(img, model.ViTnet, device)\n",
    "# # Load the weights for the ViT models\n",
    "# ourModel.ViTnet.load_state_dict(torch.load(vitnet_paths[0], map_location=device_our))\n",
    "# theirModel.ViTnet.load_state_dict(torch.load(vitnet_paths[1], map_location=device_their))\n",
    "# baselineModel.ViTnet.load_state_dict(torch.load(vitnet_paths[2], map_location=device_baseline))\n",
    "\n",
    "# # Load the weights for the agents\n",
    "# ourModel.agent.q_network.load_state_dict(torch.load(agent_paths[0], map_location=device_our))\n",
    "# theirModel.agent.q_network.load_state_dict(torch.load(agent_paths[1], map_location=device_their))\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
